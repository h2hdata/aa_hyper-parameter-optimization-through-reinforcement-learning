{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter Optimization through Reinforcement Learning approch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we are using reward and action approch of reinforcement learning. In this increment in metic value corresponds to reward where as change in hyperparameter are due to action taken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is for hyperparameter optimization.\n",
    "Data: tik-tok data\n",
    "Models: Feed-forward Neural Network with reward and action concept.\n",
    "\n",
    "#Copyright H2HDATA\n",
    "\n",
    "The entire prcess occurs in seven stages-\n",
    "1. DATA INGESTION\n",
    "2. DATA ANALYSIS \n",
    "3. DATA MUNGING\n",
    "4. DATA EXPLORATION\n",
    "5. DATA MODELING\n",
    "6. HYPER-PARAMETERS OPTIMIZATION\n",
    "7. PREDICTION\n",
    "8. VISUAL ANALYSIS\n",
    "9. RESULTS\n",
    "\n",
    "\n",
    "Used library\n",
    "1. pandas\n",
    "2. numpy\n",
    "3. time\n",
    "4. sys\n",
    "5. keras\n",
    "6. matplotlib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helper\n",
    "import model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data,target = helper._read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a  b  c  d  e  f  g  h  i\n",
       "213  2  1  0  1  2  0  0  0  2\n",
       "18   2  2  2  1  2  0  1  1  0\n",
       "177  2  1  1  1  0  0  2  2  2\n",
       "782  1  2  1  1  2  2  1  0  2\n",
       "852  1  0  2  2  1  2  0  0  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213    1\n",
       "18     1\n",
       "177    1\n",
       "782    0\n",
       "852    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Columns = 9\n",
    "2. Rows = 958\n",
    "3. target_values = 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 958 entries, 213 to 320\n",
      "Data columns (total 9 columns):\n",
      "a    958 non-null int64\n",
      "b    958 non-null int64\n",
      "c    958 non-null int64\n",
      "d    958 non-null int64\n",
      "e    958 non-null int64\n",
      "f    958 non-null int64\n",
      "g    958 non-null int64\n",
      "h    958 non-null int64\n",
      "i    958 non-null int64\n",
      "dtypes: int64(9)\n",
      "memory usage: 74.8 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHOlJREFUeJzt3XuMXOWZ5/HvD5ubDIGAo15id9LM\nQJC8YXbAHSCLNtuBSMPFE5PZcIkQgcgra7SgMYJRMNmRdmYVaeGPQAiTyYwnsIZZD5cEds2QaBkG\n3EqyS0hsh3CzcIzHLPY4cQhg3EYEGZ7947xtl9vdVdXVVX3Oeev3kUp9bq563E+9T5/L+56jiMDM\nzPJ1WNkBmJlZb7nQm5llzoXezCxzLvRmZplzoTczy5wLvZlZ5lzozSYhaZukz5Qdh1k3uNCbmWXO\nhd7MLHMu9B2StFLSy5L2SHpR0ufKjsm67hMpt29I+u+Sjio7IOsuSYOSHpb0a0m/kfSXZcfUCy70\nnXsZ+HfAccBfAP9D0knlhmRddiXwB8DvAh8D/qzccKybJM0BHgVeAYaABcD9ZcbUK/K9brpD0jPA\nf4mItWXHYjMnaRtwS0T8dZq/CLgzIn631MCsayR9EngEOCki9pUdTy95j75Dkr4o6RlJb0p6E/g4\nML/suKyrXm2YfgX4cFmBWE8MAq/kXuQB5pYdQB1J+ijwt8D5wFMR8V7ao1e5kVmXDTZMfwT4l7IC\nsZ54FfiIpLm5F3vv0XdmHhDArwEkfYlij97ycq2khZJOAP4z8EDZAVlX/QTYCdwiaZ6koySdW3ZQ\nveBC34GIeBH4GvAU8CvgdOD/lBqU9cLfA/8IbKW4+P7VcsOxboqI94A/BE4B/h+wHbi81KB6xBdj\nzcwy5z16M7PMudDbfpLmSPqZpEfT/MmSnpa0RdIDko5Iy49M81vS+qEy4zaz5lzordEKYFPD/K3A\n7RFxCvAGsCwtXwa8kZbfnrYzs4pyoTcAJC0ELga+neYFnAd8N21yD3BJml6a5knrz0/bm1kFVaIf\n/fz582NoaGj//N69e5k3b155AU1DXWKdGOeGDRtei4gPNWzydeDLwLFp/kTgzYb+xdsphoiTfr4K\nEBH7JO1O27/W+JmSlgPLAY4++ujFg4MHuqW///77HHZY9fcz6hInHBrr5s2bJ+a4p+rajusSJ7TV\njicXEaW/Fi9eHI3WrVsXdVGXWCfGCayP9PsHlgB/laZHKO7/MR/Y0rDNIPB8mn4eWNiw7mVgfmSY\n47rEGdE8x7Pxco57r9McV2KP3kp3LvDZdD+Xo4APAHcAxzeMGlwI7Ejb76Ao/NslzaW4sdtvZj9s\nM2tH24U+3eltPbAjIpZIOpniTm8nAhuAqyLiXUlHAvcCiyka/+URsa3rkdt+Qyu/13Kb1RdMfWga\nETcDNwNIGgH+NCKulPQd4PMUeb4aGL9h2yNp/qm0/sm0d2E9MtMcj3M7rq5u5Xgy0zn56B4Z/ecm\n4AZJWygKwV1p+V3AiWn5DcDKkuKz6XM77kNtFXr3yOgfETEaEUvS9NaIOCsiTomISyPit2n5O2n+\nlLR+a7lRWzvcjvtXu6duetojY2BggNHR0f3rxsbGDpqvsirEeuPprW+8V4U4rXRux5OoSpy9bMct\nC72kJcCuiNiQzt92RUSsAlYBDA8Px8jIgbceHR2lcb7KqhDrNW2e2ys7TiuP2/HUqhJnL9txO3v0\n7pFhVn9ux32s5Tn6iLg5IhZGxBBwBUUPiyuBdRQ9LmDyHhngHhlmleB23N9mMuTPPTLM6s/tuA9M\na8BURIwCo2l6K3DWJNu8A1zahdjMSvfcjt0tz51uu+XiWYqmO9yO+089buJhZmYdc6E3M8ucC72Z\nWeZ8U7MZanUOt27nb836TY7XYSbyHr2ZWeZc6M3MMudCb2aWORd6M7PMudCbmWXOhd7MLHMu9GZm\nmXM/eitFP/RdNqsK79GbmWXOhd7MLHMu9GZmmXOhNyQNSlon6UVJL0hakZafIOlxSb9IPz+YlkvS\nNyRtkfSspDPL/R+YWTMu9AawD7gxIhYB5wDXSlpE8VShJyLiVOAJDjxl6ELg1PRaDnxr9kM2s3a5\n0BsRsTMiNqbpPcAmYAGwFLgnbXYPcEmaXgrcG4UfUzxg+qRZDtvM2tSye6WkQeBeYAAIYFVE3CHp\nBOABYAjYBlwWEW9IEsXT5S8C3gauGS8iVn2ShoAzgKeBgYjYmVb9kuI7AMUfgVcb/tn2tGxnwzIk\nLafY42dgYIDR0dH96waOhhtP39c0lsbty1KVOFvFADA2NjZlLG7H/a2dfvTjh/UbJR0LbJD0OHAN\nxWH9LZJWUhzW38TBh/VnUxzWn92L4K27JB0DPARcHxFvFW29EBEhKabzfhGxClgFMDw8HCMjI/vX\n3blmLV97rvnXb9uVI03Xz4aqxNlqzAHA6gvm0fg7nsDtuI+1PHXjw/r+IOlwiiK/JiIeTot/NZ67\n9HNXWr4DGGz45wvTMqsot+P+Nq2RsbN1WL/r9d3cuWZt01hOX3DcdELvmVaH9jU5rBdwF7ApIm5r\nWPUIcDVwS/q5tmH5dZLup9jL293wXbCKczs+WC6n55ppu9D7sH5yrWKtyWH9ucBVwHOSnknLvkJR\n4B+UtAx4Bbgsrfs+xbnbLRTnb7/Uaew2u9yOD1WVOLvQjqfUVqFvdlgfETt9WF9vEfEjQFOsPn+S\n7QO4tqdBWde5Hfevlufo2zish0MP67+YBtWcgw/rzUrndtzf2tmj92G9Wf25HfexloXeh/Vm9ed2\n3N88MtbMLHMu9GZmmXOhNzPLnAu9mVnmXOjNzDLnQm9mljkXejOzzLnQm5llzoXezCxzLvRmZplz\noTczy5wLvZlZ5lzozcwy50JvZpY5F3ozs8y50JuZZc6F3swscy70ZmaZc6E3M8tcTwq9pAskvSRp\ni6SVvfgMK5dznD/nOB9dL/SS5gDfBC4EFgFfkLSo259j5XGO8+cc56UXe/RnAVsiYmtEvAvcDyzt\nwedYeZzj/DnHGZnbg/dcALzaML8dOHviRpKWA8vT7JiklxpWzwdea/YhunWGUXZP01irEuenbz0k\nzo/O4O36Kcd1idM57lxd4uw4x70o9G2JiFXAqsnWSVofEcOzHFJH6hJrGXHmkOO6xAnOcafqEid0\nHmsvTt3sAAYb5hemZZYP5zh/znFGelHofwqcKulkSUcAVwCP9OBzKkPSaklfLTuOWdR3Oe5DfZFj\nSacBiyTtkfQnZcfTK10/dRMR+yRdBzwGzAHujogXpvk2kx4KVtQq4N+WHUQbuvY77bMc1yVOcI47\n8WXg/0bEZ8oOpE0d/U4VEd0OpO9IWg1sj4g/KzsWM2ufpH8C7o+Ib5cdSy95ZGwHJJ0haWM63HsA\nOKrsmKy7JH1Y0kOSfi3pn3M+rO9Xkp4EPg38paQxSR8rO6ZecaGfpnS+8n8BfwecAHwH+A+lBmVd\nJekw4B+An1N0MzwfuF7SH5QamHVVRJwH/BC4LiKOiYjNZcfUK5Ur9DUYdn0OcDhwOkUvhD+nuHBV\nOZIGJa2T9KKkFyStKDsmqEWOPwGcBlwHbIyIrcDfUlyQrBTnuHOS7qa4vvYXZcfSTFdyHBGVeVFc\n9HkZ+B3gCIo9qkVlxzUhxisoCvungDOB54H7gK+WHdsksZ4EnJmmjwU2l/37rEmOLwP2AXuA94A3\n0/T3y47NOe5qnJ8C1gM7yo6l1zmu2h59HYZd76Q4nP8h8Hpa9pHywplaROyMiI1peg+wiSL2MtUh\nx68C/0xx1LYpIo6PiGMj4qKS4zqEc9y5iPgBxR/0SutGjqtW6Ccbdl32l3aipyi+HH9C0T31AxRf\n7EqTNAScATxdbiS1yPFPKPbg/xiQpDmSPi7pEyXH1ZRznL9Oc1y1Ql95aQ/lj4BrgGeA44CHy4yp\nFUnHAA8B10fEW2XHU3UR8R6whOKujR+juLfItylyXUnOcf5mkuPS7nUzhVoMu46I9cAZ6a/roxFx\nebkRTU3S4RRfjjURUYU/SHXJ8b+kLpWPRsTHy46nGed4Rq4AHi07iFZmmuO29+jT4evPJD2a5k+W\n9HS6qv5A6naIpCPT/Ja0fmga8fTFsOvZIknAXRTnmW8rO57EOe6i6ebY7bh+utGOp3PqZgXFRYBx\ntwK3R8QpwBvAsrR8GfBGWn572q4tEbGPokvbY+mzHozpD7ueFZLuozhff5qk7ZKWtfo3JTgXuAo4\nT9Iz6VXqBcW65Lgm+YXp59jtOMk4x4dqs3vPQuAJ4DyKwxxRnLecm9Z/EngsTT8GfDJNz03bqewu\nSn751e8vt+P+fbW7R/91ipv/vJ/mTwTejOIvNxx8VX3/Ffe0fnfa/iCSlktan17LJ643s65zO+5T\nLS/GSloC7IqIDZJGuvXB0fDAgvnz58fw8PDfjK/bu3cv8+bN69ZH9VRdYp0Y54YNG16LiA/N1ufP\nnz8/hoaGpoynquoSJzTPsdvx1OoSJ8ygHbfa5Qf+G8Vf+m3AL4G3gTV08ZBv8eLF0WjdunVRF3WJ\ndWKcwPqYxUPHuua4LnFGNM+x2/HU6hJnROftuOWpm4i4OSIWRsQQxdXzJyPiSmAd8Pm02dXA2jT9\nSJonrX8yBWRmJXE77m8z6Ud/E3C/iicr/Yyi+w/p599J2kJxi4DK3QgqN0Mrv9dym9UX1OPQ1CbX\nwxy7HVdEL9vxtAp9RIwCo2l6K5MM/Y+Id4BLO4rGzHrO7bj/+BYIZmaZc6E3M8ucC73tN0vD481s\nlrnQW6OeD483s9nnQm8ASFoIXExxO97xGymdB3w3bXIPcEmaXprmSevPT9ubWQVV7TbFtfPcjt1c\n06Rb1LZbLp7FaGZkfHj8sWm+7eHxksaHx7/W+IZpSPxygIGBAUZHR/evGxsbO2i+qqoS542nt34Q\nUlVirZtWbRhq1Y4n5UJvszI8fnh4OEZGDrz16OgojfNVVZU4WxUiKPpYVyFWqx4XeoPiNqifTbc+\nPYri8Yh3AMdLmpv26hsfHjH+YIntkuZSPHnpN7Mftpm1w+fozcPjzTLnQm/N3ATckIbBn8jBw+NP\nTMtvAFaWFJ+ZtcGnbuwgHh5vlh/v0ZuZZc6F3swscz51Y9ZEP/Sxtvx5j97MLHMu9GZmmXOhNzPL\nXCXP0fu8qFn9uR1Xh/fozcwy50JvZpY5F3ozs8y50JuZZa5loZc0KGmdpBclvSBpRVp+gqTHJf0i\n/fxgWi5J30jPE31W0pm9/k+YWXNux/2tnT36fcCNEbEIOAe4VtIiijsWPhERpwJPcOAOhhcCp6bX\ncuBbXY/azKbL7biPtSz0EbEzIjam6T0UD49ewMHPDZ34PNF7o/BjiodXnNT1yM2sbW7H/W1a/egl\nDQFnAE8DAxGxM636JTCQpvc/TzQZf9bozoZlTZ8nOnB062dkVuXZmK1inY04/TxRmw6344NVJc5e\ntuO2C72kY4CHgOsj4i1J+9dFREia1hOGmj1P9M41a/nac81D23blSNP1s6VVrLMRp58nau1yOz5U\nVeLsZTtuq9eNpMMpvhxrIuLhtPhX44dy6eeutHz8eaLjGp81ahXkC3X9we24f7XT60YUj47bFBG3\nNaxqfG7oxOeJfjEVg3OA3Q2HhlZNvlCXObfj/tbOqZtzgauA5yQ9k5Z9BbgFeFDSMuAV4LK07vvA\nRcAW4G3gS12N2LouNeCdaXqPpMYLdSNps3soHjF4Ew0X6oAfSzpe0kkuBJXmdtzHWhb6iPgRoClW\nnz/J9gFcO8O4rCSzdaGuLheHc7lQ53bc3yp590orx2xeqBsdHa3FxeF+uFBn+fMtEAzwhTqznLnQ\nmy/UmWXOp24MfKHOLGsu9FbKhTo/fchs9vjUjZlZ5lzozcwy50JvZpY5F3ozs8y50JuZZc6F3sws\ncy70ZmaZc6E3M8ucC72ZWeZc6M3MMudCb2aWORd6M7PMudCbmWXOhd7MLHMu9GZmmXOhNzPLXE8K\nvaQLJL0kaYuklb34DCuXc5w/5zgfXS/0kuYA3wQuBBYBX5C0qNufY+VxjvPnHOelF3v0ZwFbImJr\nRLwL3A8s7cHnWHmc4/w5xxnpxTNjFwCvNsxvB86euJGk5cDyNDsm6aWG1fOB15p9iG6dYZTd0zTW\nqsT56VsPifOjM3i7fspxXeJ0jjtXlzg7znFpDwePiFXAqsnWSVofEcOzHFJH6hJrGXHmkOO6xAnO\ncafqEid0HmsvTt3sAAYb5hemZZYP5zh/znFGelHofwqcKulkSUcAVwCP9OBzrDzZ51jSNkmfKTuO\nEmWf4wb/WtJI2UH0UtdP3UTEPknXAY8Bc4C7I+KFab7NpIeCFVWXWLsWZ5/luC5xgnPcqRURMVp2\nEG3q6HeqiOh2IGa1J2kb8B8j4p/KjsVspjwytgOSzpT0M0l7JH1H0gOSvlp2XNZ1vy/pWUm7U46P\nKjsg675+OE3nQj9N6Xzl/wRWAycA9wGfKzMm65nLgAuAk4HfA64pNRqzDlWu0Ndg2PU5FNc2/g1F\nL4T/Cvyk1IimIGlQ0jpJL0p6QdKKsmOCWuQY4ETgNOAfI+J14B+A3y83pEM5x52TdLekXcCHy46l\nmW7kuFKFvibDrj9MUeBXU+ztwcEDS6pkH3BjRCyi+AN1bdm/z5rkGGAMuKlh/m3gmJJiacY57txq\nDrThKptxjitV6KnHsOudFKMGfwi8npYNTr15eSJiZ0RsTNN7gE0UsZepDjkG+C2wp+wgWnGOOxcR\nP+BAG66sbuS4aoV+smHXZX9pJ3oKeA+4jqLb2bEUX+xKkzQEnAE8XW4ktchxLTnH+es0x1Ur9JWX\n9lD+CFgGPAscDzxKsQdYSZKOAR4Cro+It8qOx7rPOc7fTHJc2r1uplCLYdcRsZ6i690QRZFfQHGx\nrnIkHU7x5VgTEQ+XHQ/1yfFQyu/4/J+XFkwLzvGMba76eImZ5rhqe/S1GHYt6d9L+lcUp26Op+h6\n97/LjepQkgTcBWyKiNvKjiepRY7rwjnOXzdyXKlCHxH7KM59P0ZxweHBDoZdz4bTgG3ALyj25vcC\nF5UZ0BTOBa4CzpP0THpNGaekOWkg2KNp/mRJT6cucg+kRoukI9P8lrR+qN2A6pJjSfdRXI85TdJ2\nScvKjmkK08rxbHCOu27GOfYtEGw/STcAw8AHImKJpAeBhyPifkl/Dfw8Ir4l6T8BvxcRfyzpCuBz\nEXF5mbGb2dQqtUdv5ZG0ELgY+HaaF3Ae8N20yT3AJWl6aZonrT8/bW9mFVSJi7Hz58+PoaGh/fN7\n9+5l3rx55QU0DXWJdWKcGzZseC0iPtSwydeBL1N0F4ViZOib6TAcDu4it7/7XLrL4e60/UFP6VHD\n04eOPvroxYODB67Pvf/++xx2WPX3M+oSJxwa6+bNmyfmuKfq2o7rEie01Y4nFxGlvxYvXhyN1q1b\nF3VRl1gnxgmsj/T7B5YAf5WmRyh6Es2nGPQyvs0g8Hyafh5Y2LDuZWB+ZJjjusQZ0TzHs/Fyjnuv\n0xxXYo/eSncu8Nl0geco4APAHcDxkuZGsVff2EVuvPvcdklzgeOA38x+2GbWjrYLfbp/xXpgRxQX\n6k6mGNp8IrABuCoi3pV0JHAvsJii8V8eEdu6HrntN7Tyey23WX3B1IemEXEzcDOAiift/GlEXCnp\nO8DnKfJ8NbA2/ZNH0vxTaf2Tae/CemSmOR7ndlxd3crxZKZz8nEFRVepcbcCt0fEKcAbFCNFST/f\nSMtvT9tZPd0E3CBpC0UhuCstvws4MS2/Aajk3QltUm7HfaitQu8eGf0jIkYjYkma3hoRZ0XEKRFx\naUT8Ni1/J82fktZvLTdqa4fbcf9q99RNT3tkDAwMMDo6un/d2NjYQfNVVoVYbzx9X8ttqhCnlc7t\neBJVibOX7bhloZe0BNgVERvUxSelR8Qq0oNuh4eHY2TkwFuPjo7SOF9lVYj1mjbP7ZUdp5XH7Xhq\nVYmzl+24nT1698gwqz+34z7W8hx9RNwcEQsjYoji5kRPRsSVwDqKHhcweY8McI8Ms0pwO+5vMxny\n5x4ZZvXndtwHpjVgKiJGgdE0vZVJnqwUEe8Al3YhNrPSPbdjd8tzp9tuuXiWoukOt+P+U4+beJiZ\nWcdc6M3MMudCb2aWOd/UbIZancOt2/lbs36T43WYibxHb2aWORd6M7PMudCbmWXOhd7MLHMu9GZm\nmXOhNzPLnAu9mVnm3I/eStEPfZfNqsJ79GZmmXOhNzPLnAu9mVnmXOgNSYOS1kl6UdILklak5SdI\nelzSL9LPD6blkvQNSVskPSvpzHL/B2bWjAu9AewDboyIRcA5wLWSFlE8VeiJiDgVeIIDTxm6EDg1\nvZYD35r9kM2sXS70RkTsjIiNaXoPsAlYACwF7kmb3QNckqaXAvdG4ccUD5g+aZbDNrM2texeKWkQ\nuBcYAAJYFRF3SDoBeAAYArYBl0XEG5JE8XT5i4C3gWvGi4hVn6Qh4AzgaWAgInamVb+k+A5A8Ufg\n1YZ/tj0t29mwDEnLKfb4GRgYYHR0dP+6gaPhxtP3NY2lcfuyVCXOVjEAjI2NTRmL23F/a6cf/fhh\n/UZJxwIbJD0OXENxWH+LpJUUh/U3cfBh/dkUh/Vn9yJ46y5JxwAPAddHxFtFWy9EREiK6bxfRKwC\nVgEMDw/HyMjI/nV3rlnL155r/vXbduVI0/WzoSpxthpzALD6gnk0/o4ncDvuYy1P3fiwvj9IOpyi\nyK+JiIfT4l+N5y793JWW7wAGG/75wrTMKsrtuL9Na2TsbB3W73p9N3euWds0ltMXHDed0Hum1aF9\nTQ7rBdwFbIqI2xpWPQJcDdySfq5tWH6dpPsp9vJ2N3wXrOLcjg+Wy+m5Ztou9D6sn1yrWGtyWH8u\ncBXwnKRn0rKvUBT4ByUtA14BLkvrvk9x7nYLxfnbL3Uau80ut+NDVSXOLrTjKbVV6Jsd1kfETh/W\n11tE/AjQFKvPn2T7AK7taVDWdW7H/avlOfo2Duvh0MP6L6ZBNefgw3qz0rkd97d29uh9WG9Wf27H\nfaxlofdhvVn9uR33N4+MNTPLnAu9mVnmXOjNzDLnQm9mljkXejOzzLnQm5llzoXezCxzLvRmZplz\noTczy5wLvZlZ5lzozcwy50JvZpY5F3ozs8y50JuZZc6F3swscy70ZmaZc6E3M8ucC72ZWeZc6M3M\nMteTQi/pAkkvSdoiaWUvPsPK5RznzznOR9cLvaQ5wDeBC4FFwBckLer251h5nOP8Ocd56cUe/VnA\nlojYGhHvAvcDS3vwOVYe5zh/znFG5vbgPRcArzbMbwfOnriRpOXA8jQ7JumlhtXzgdeafYhunWGU\n3dM01qrE+elbD4nzozN4u37KcV3idI47V5c4O85xLwp9WyJiFbBqsnWS1kfE8CyH1JG6xFpGnDnk\nuC5xgnPcqbrECZ3H2otTNzuAwYb5hWmZ5cM5zp9znJFeFPqfAqdKOlnSEcAVwCM9+Bwrj3OcP+c4\nI10/dRMR+yRdBzwGzAHujogXpvk2kx4KVlRdYu1anH2W47rECc5xp+oSJ3QYqyKi24GYmVmFeGSs\nmVnmXOjNzDJXuUJfl2HXku6WtEvS82XHMhVJg5LWSXpR0guSVpQdE9Qjx3XILzjHM9FXOY6Iyrwo\nLvq8DPwOcATwc2BR2XFNEeungDOB58uOpUmMJwFnpuljgc1l/z7rkuM65Nc5do7bfVVtj742w64j\n4gfA62XH0UxE7IyIjWl6D7CJYsRjmWqR4zrkF5zjmeinHFet0E827LrsL20WJA0BZwBPlxuJc9wr\nznH+Os1x1Qq99YCkY4CHgOsj4q2y47Huc47zN5McV63Qe9h1l0k6nOLLsSYiHi47HpzjrnOO8zfT\nHFet0HvYdRdJEnAXsCkibis7nsQ57iLnOH/dyHGlCn1E7APGh11vAh6M6Q+7nhWS7gOeAk6TtF3S\nsrJjmsS5wFXAeZKeSa+LygyoLjmuSX7BOe5YP+XYt0AwM8tcpfbozcys+1zozcwy50JvZpY5F3oz\ns8y50JuZZc6F3swscy70ZmaZ+/8XeMwxGtFzVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2eb8a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nX3JSiThTY81MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8e3fe90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that data having target with ration of around 65:35."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>958.000000</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>958.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.222338</td>\n",
       "      <td>1.133612</td>\n",
       "      <td>1.222338</td>\n",
       "      <td>1.133612</td>\n",
       "      <td>1.311065</td>\n",
       "      <td>1.133612</td>\n",
       "      <td>1.222338</td>\n",
       "      <td>1.133612</td>\n",
       "      <td>1.222338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.775569</td>\n",
       "      <td>0.798966</td>\n",
       "      <td>0.775569</td>\n",
       "      <td>0.798966</td>\n",
       "      <td>0.740882</td>\n",
       "      <td>0.798966</td>\n",
       "      <td>0.775569</td>\n",
       "      <td>0.798966</td>\n",
       "      <td>0.775569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                a           b           c           d           e           f  \\\n",
       "count  958.000000  958.000000  958.000000  958.000000  958.000000  958.000000   \n",
       "mean     1.222338    1.133612    1.222338    1.133612    1.311065    1.133612   \n",
       "std      0.775569    0.798966    0.775569    0.798966    0.740882    0.798966   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000    0.000000    1.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "75%      2.000000    2.000000    2.000000    2.000000    2.000000    2.000000   \n",
       "max      2.000000    2.000000    2.000000    2.000000    2.000000    2.000000   \n",
       "\n",
       "                g           h           i  \n",
       "count  958.000000  958.000000  958.000000  \n",
       "mean     1.222338    1.133612    1.222338  \n",
       "std      0.775569    0.798966    0.775569  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      1.000000    0.000000    1.000000  \n",
       "50%      1.000000    1.000000    1.000000  \n",
       "75%      2.000000    2.000000    2.000000  \n",
       "max      2.000000    2.000000    2.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that data is not skewed and have mean of around 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "574/574 [==============================] - 0s 47us/step - loss: 0.7246 - binary_accuracy: 0.3990\n",
      "Epoch 2/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6492 - binary_accuracy: 0.6498\n",
      "Epoch 3/100\n",
      "574/574 [==============================] - 0s 29us/step - loss: 0.6614 - binary_accuracy: 0.6498\n",
      "Epoch 4/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6567 - binary_accuracy: 0.6498\n",
      "Epoch 5/100\n",
      "574/574 [==============================] - 0s 29us/step - loss: 0.6469 - binary_accuracy: 0.6498\n",
      "Epoch 6/100\n",
      "574/574 [==============================] - 0s 41us/step - loss: 0.6485 - binary_accuracy: 0.6498\n",
      "Epoch 7/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6485 - binary_accuracy: 0.6498\n",
      "Epoch 8/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6471 - binary_accuracy: 0.6498\n",
      "Epoch 9/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6468 - binary_accuracy: 0.6498\n",
      "Epoch 10/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6468 - binary_accuracy: 0.6498\n",
      "Epoch 11/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6466 - binary_accuracy: 0.6498\n",
      "Epoch 12/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6464 - binary_accuracy: 0.6498\n",
      "Epoch 13/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6473 - binary_accuracy: 0.6498\n",
      "Epoch 14/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6461 - binary_accuracy: 0.6498\n",
      "Epoch 15/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6468 - binary_accuracy: 0.6498\n",
      "Epoch 16/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6459 - binary_accuracy: 0.6498\n",
      "Epoch 17/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6462 - binary_accuracy: 0.6498\n",
      "Epoch 18/100\n",
      "574/574 [==============================] - 0s 21us/step - loss: 0.6459 - binary_accuracy: 0.6498\n",
      "Epoch 19/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6455 - binary_accuracy: 0.6498\n",
      "Epoch 20/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6453 - binary_accuracy: 0.6498\n",
      "Epoch 21/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6457 - binary_accuracy: 0.6498\n",
      "Epoch 22/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6448 - binary_accuracy: 0.6498\n",
      "Epoch 23/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6449 - binary_accuracy: 0.6498\n",
      "Epoch 24/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6449 - binary_accuracy: 0.6498\n",
      "Epoch 25/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6457 - binary_accuracy: 0.6498\n",
      "Epoch 26/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6434 - binary_accuracy: 0.6498\n",
      "Epoch 27/100\n",
      "574/574 [==============================] - 0s 30us/step - loss: 0.6428 - binary_accuracy: 0.6498\n",
      "Epoch 28/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6427 - binary_accuracy: 0.6498\n",
      "Epoch 29/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6426 - binary_accuracy: 0.6498\n",
      "Epoch 30/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6413 - binary_accuracy: 0.6498\n",
      "Epoch 31/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6420 - binary_accuracy: 0.6498\n",
      "Epoch 32/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6418 - binary_accuracy: 0.6498\n",
      "Epoch 33/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6401 - binary_accuracy: 0.6498\n",
      "Epoch 34/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6392 - binary_accuracy: 0.6498\n",
      "Epoch 35/100\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.5865 - binary_accuracy: 0.744 - 0s 29us/step - loss: 0.6383 - binary_accuracy: 0.6498\n",
      "Epoch 36/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6380 - binary_accuracy: 0.6498\n",
      "Epoch 37/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6369 - binary_accuracy: 0.6498\n",
      "Epoch 38/100\n",
      "574/574 [==============================] - 0s 28us/step - loss: 0.6362 - binary_accuracy: 0.6498\n",
      "Epoch 39/100\n",
      "574/574 [==============================] - 0s 33us/step - loss: 0.6355 - binary_accuracy: 0.6498\n",
      "Epoch 40/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6345 - binary_accuracy: 0.6498\n",
      "Epoch 41/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6334 - binary_accuracy: 0.6498\n",
      "Epoch 42/100\n",
      "574/574 [==============================] - 0s 28us/step - loss: 0.6321 - binary_accuracy: 0.6498\n",
      "Epoch 43/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6310 - binary_accuracy: 0.6498\n",
      "Epoch 44/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6308 - binary_accuracy: 0.6498\n",
      "Epoch 45/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6285 - binary_accuracy: 0.6498\n",
      "Epoch 46/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6292 - binary_accuracy: 0.6498\n",
      "Epoch 47/100\n",
      "574/574 [==============================] - 0s 31us/step - loss: 0.6262 - binary_accuracy: 0.6498\n",
      "Epoch 48/100\n",
      "574/574 [==============================] - 0s 34us/step - loss: 0.6269 - binary_accuracy: 0.6498\n",
      "Epoch 49/100\n",
      "574/574 [==============================] - 0s 28us/step - loss: 0.6255 - binary_accuracy: 0.6498\n",
      "Epoch 50/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6227 - binary_accuracy: 0.6498\n",
      "Epoch 51/100\n",
      "574/574 [==============================] - 0s 28us/step - loss: 0.6232 - binary_accuracy: 0.6690\n",
      "Epoch 52/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6207 - binary_accuracy: 0.6655\n",
      "Epoch 53/100\n",
      "574/574 [==============================] - 0s 29us/step - loss: 0.6202 - binary_accuracy: 0.6551\n",
      "Epoch 54/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6195 - binary_accuracy: 0.6568\n",
      "Epoch 55/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6181 - binary_accuracy: 0.6690\n",
      "Epoch 56/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6170 - binary_accuracy: 0.6725\n",
      "Epoch 57/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6173 - binary_accuracy: 0.6707\n",
      "Epoch 58/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6155 - binary_accuracy: 0.6672\n",
      "Epoch 59/100\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.6845 - binary_accuracy: 0.566 - 0s 32us/step - loss: 0.6145 - binary_accuracy: 0.6690\n",
      "Epoch 60/100\n",
      "574/574 [==============================] - 0s 28us/step - loss: 0.6146 - binary_accuracy: 0.6707\n",
      "Epoch 61/100\n",
      "574/574 [==============================] - 0s 29us/step - loss: 0.6126 - binary_accuracy: 0.6672\n",
      "Epoch 62/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6128 - binary_accuracy: 0.6672\n",
      "Epoch 63/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6119 - binary_accuracy: 0.6760\n",
      "Epoch 64/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6117 - binary_accuracy: 0.6707\n",
      "Epoch 65/100\n",
      "574/574 [==============================] - 0s 29us/step - loss: 0.6099 - binary_accuracy: 0.6690\n",
      "Epoch 66/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6125 - binary_accuracy: 0.6899\n",
      "Epoch 67/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6077 - binary_accuracy: 0.6760\n",
      "Epoch 68/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6090 - binary_accuracy: 0.6777\n",
      "Epoch 69/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6070 - binary_accuracy: 0.6812\n",
      "Epoch 70/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6064 - binary_accuracy: 0.6969\n",
      "Epoch 71/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6035 - binary_accuracy: 0.6951\n",
      "Epoch 72/100\n",
      "574/574 [==============================] - 0s 31us/step - loss: 0.6035 - binary_accuracy: 0.6777\n",
      "Epoch 73/100\n",
      "574/574 [==============================] - 0s 35us/step - loss: 0.6020 - binary_accuracy: 0.6934\n",
      "Epoch 74/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.5993 - binary_accuracy: 0.7003\n",
      "Epoch 75/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6001 - binary_accuracy: 0.6847\n",
      "Epoch 76/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6004 - binary_accuracy: 0.6934\n",
      "Epoch 77/100\n",
      "574/574 [==============================] - 0s 37us/step - loss: 0.5979 - binary_accuracy: 0.7073\n",
      "Epoch 78/100\n",
      "574/574 [==============================] - 0s 44us/step - loss: 0.5950 - binary_accuracy: 0.7021\n",
      "Epoch 79/100\n",
      "574/574 [==============================] - 0s 30us/step - loss: 0.5946 - binary_accuracy: 0.7213\n",
      "Epoch 80/100\n",
      "574/574 [==============================] - 0s 28us/step - loss: 0.5924 - binary_accuracy: 0.7125\n",
      "Epoch 81/100\n",
      "574/574 [==============================] - 0s 46us/step - loss: 0.5905 - binary_accuracy: 0.7195\n",
      "Epoch 82/100\n",
      "574/574 [==============================] - 0s 31us/step - loss: 0.5894 - binary_accuracy: 0.7247\n",
      "Epoch 83/100\n",
      "574/574 [==============================] - 0s 29us/step - loss: 0.5879 - binary_accuracy: 0.7195\n",
      "Epoch 84/100\n",
      "574/574 [==============================] - 0s 36us/step - loss: 0.5863 - binary_accuracy: 0.7230\n",
      "Epoch 85/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.5850 - binary_accuracy: 0.7125\n",
      "Epoch 86/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.5831 - binary_accuracy: 0.7143\n",
      "Epoch 87/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.5823 - binary_accuracy: 0.7247\n",
      "Epoch 88/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.5816 - binary_accuracy: 0.7265\n",
      "Epoch 89/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.5801 - binary_accuracy: 0.7317\n",
      "Epoch 90/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.5784 - binary_accuracy: 0.7213\n",
      "Epoch 91/100\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.5920 - binary_accuracy: 0.711 - 0s 22us/step - loss: 0.5747 - binary_accuracy: 0.7334\n",
      "Epoch 92/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.5753 - binary_accuracy: 0.7404\n",
      "Epoch 93/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.5730 - binary_accuracy: 0.7247\n",
      "Epoch 94/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.5722 - binary_accuracy: 0.7369\n",
      "Epoch 95/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.5684 - binary_accuracy: 0.7352\n",
      "Epoch 96/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.5684 - binary_accuracy: 0.7265\n",
      "Epoch 97/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.5671 - binary_accuracy: 0.7352\n",
      "Epoch 98/100\n",
      "574/574 [==============================] - 0s 30us/step - loss: 0.5660 - binary_accuracy: 0.7300\n",
      "Epoch 99/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.5651 - binary_accuracy: 0.7317\n",
      "Epoch 100/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.5608 - binary_accuracy: 0.7317\n",
      "574/574 [==============================] - 0s 33us/step\n",
      "Epoch 1/100\n",
      "574/574 [==============================] - 0s 42us/step - loss: 0.8967 - binary_accuracy: 0.5976\n",
      "Epoch 2/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.7361 - binary_accuracy: 0.5557\n",
      "Epoch 3/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6605 - binary_accuracy: 0.6516\n",
      "Epoch 4/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6481 - binary_accuracy: 0.6411\n",
      "Epoch 5/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6391 - binary_accuracy: 0.6481\n",
      "Epoch 6/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6239 - binary_accuracy: 0.6516\n",
      "Epoch 7/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6230 - binary_accuracy: 0.6551\n",
      "Epoch 8/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6206 - binary_accuracy: 0.6533\n",
      "Epoch 9/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6275 - binary_accuracy: 0.6516\n",
      "Epoch 10/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6196 - binary_accuracy: 0.6690\n",
      "Epoch 11/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6191 - binary_accuracy: 0.6620\n",
      "Epoch 12/100\n",
      "574/574 [==============================] - 0s 21us/step - loss: 0.6200 - binary_accuracy: 0.6707\n",
      "Epoch 13/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6174 - binary_accuracy: 0.6916\n",
      "Epoch 14/100\n",
      "574/574 [==============================] - 0s 20us/step - loss: 0.6171 - binary_accuracy: 0.6847\n",
      "Epoch 15/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6184 - binary_accuracy: 0.6847\n",
      "Epoch 16/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6180 - binary_accuracy: 0.6777\n",
      "Epoch 17/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6183 - binary_accuracy: 0.6882\n",
      "Epoch 18/100\n",
      "574/574 [==============================] - 0s 33us/step - loss: 0.6180 - binary_accuracy: 0.6794\n",
      "Epoch 19/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6200 - binary_accuracy: 0.6794\n",
      "Epoch 20/100\n",
      "574/574 [==============================] - 0s 33us/step - loss: 0.6173 - binary_accuracy: 0.6829\n",
      "Epoch 21/100\n",
      "574/574 [==============================] - 0s 37us/step - loss: 0.6176 - binary_accuracy: 0.6812\n",
      "Epoch 22/100\n",
      "574/574 [==============================] - 0s 28us/step - loss: 0.6178 - binary_accuracy: 0.6829\n",
      "Epoch 23/100\n",
      "574/574 [==============================] - 0s 21us/step - loss: 0.6228 - binary_accuracy: 0.6829\n",
      "Epoch 24/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6199 - binary_accuracy: 0.6847\n",
      "Epoch 25/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6172 - binary_accuracy: 0.6864\n",
      "Epoch 26/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6222 - binary_accuracy: 0.6690\n",
      "Epoch 27/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6197 - binary_accuracy: 0.6847\n",
      "Epoch 28/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6164 - binary_accuracy: 0.6934\n",
      "Epoch 29/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6164 - binary_accuracy: 0.6934\n",
      "Epoch 30/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6173 - binary_accuracy: 0.6864\n",
      "Epoch 31/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6177 - binary_accuracy: 0.6864\n",
      "Epoch 32/100\n",
      "574/574 [==============================] - 0s 29us/step - loss: 0.6171 - binary_accuracy: 0.6777\n",
      "Epoch 33/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6157 - binary_accuracy: 0.6847\n",
      "Epoch 34/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6183 - binary_accuracy: 0.6882\n",
      "Epoch 35/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6165 - binary_accuracy: 0.6916\n",
      "Epoch 36/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6163 - binary_accuracy: 0.6916\n",
      "Epoch 37/100\n",
      "574/574 [==============================] - 0s 20us/step - loss: 0.6154 - binary_accuracy: 0.6934\n",
      "Epoch 38/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6186 - binary_accuracy: 0.6760\n",
      "Epoch 39/100\n",
      "574/574 [==============================] - 0s 20us/step - loss: 0.6166 - binary_accuracy: 0.6882\n",
      "Epoch 40/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6168 - binary_accuracy: 0.6969\n",
      "Epoch 41/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6167 - binary_accuracy: 0.6916\n",
      "Epoch 42/100\n",
      "574/574 [==============================] - 0s 33us/step - loss: 0.6159 - binary_accuracy: 0.6916\n",
      "Epoch 43/100\n",
      "574/574 [==============================] - 0s 28us/step - loss: 0.6167 - binary_accuracy: 0.6794\n",
      "Epoch 44/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6169 - binary_accuracy: 0.6882\n",
      "Epoch 45/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6181 - binary_accuracy: 0.6882\n",
      "Epoch 46/100\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.6533 - binary_accuracy: 0.666 - 0s 22us/step - loss: 0.6171 - binary_accuracy: 0.6829\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 0s 23us/step - loss: 0.6174 - binary_accuracy: 0.6882\n",
      "Epoch 48/100\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.6107 - binary_accuracy: 0.655 - 0s 23us/step - loss: 0.6157 - binary_accuracy: 0.6899\n",
      "Epoch 49/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6199 - binary_accuracy: 0.6742\n",
      "Epoch 50/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6160 - binary_accuracy: 0.6969\n",
      "Epoch 51/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6189 - binary_accuracy: 0.6812\n",
      "Epoch 52/100\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.6485 - binary_accuracy: 0.666 - 0s 24us/step - loss: 0.6171 - binary_accuracy: 0.6864\n",
      "Epoch 53/100\n",
      "574/574 [==============================] - 0s 28us/step - loss: 0.6187 - binary_accuracy: 0.6829\n",
      "Epoch 54/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6192 - binary_accuracy: 0.6882\n",
      "Epoch 55/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6207 - binary_accuracy: 0.6794\n",
      "Epoch 56/100\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.6195 - binary_accuracy: 0.666 - 0s 25us/step - loss: 0.6160 - binary_accuracy: 0.6847\n",
      "Epoch 57/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6172 - binary_accuracy: 0.6847\n",
      "Epoch 58/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6187 - binary_accuracy: 0.6847\n",
      "Epoch 59/100\n",
      "574/574 [==============================] - 0s 31us/step - loss: 0.6166 - binary_accuracy: 0.6899\n",
      "Epoch 60/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6167 - binary_accuracy: 0.6934\n",
      "Epoch 61/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6162 - binary_accuracy: 0.6951\n",
      "Epoch 62/100\n",
      "574/574 [==============================] - 0s 28us/step - loss: 0.6175 - binary_accuracy: 0.6916\n",
      "Epoch 63/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6156 - binary_accuracy: 0.6916\n",
      "Epoch 64/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6167 - binary_accuracy: 0.6847\n",
      "Epoch 65/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6164 - binary_accuracy: 0.6794\n",
      "Epoch 66/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6198 - binary_accuracy: 0.6777\n",
      "Epoch 67/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6166 - binary_accuracy: 0.6760\n",
      "Epoch 68/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6185 - binary_accuracy: 0.6707\n",
      "Epoch 69/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6174 - binary_accuracy: 0.6847\n",
      "Epoch 70/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6166 - binary_accuracy: 0.6916\n",
      "Epoch 71/100\n",
      "574/574 [==============================] - 0s 21us/step - loss: 0.6161 - binary_accuracy: 0.6951\n",
      "Epoch 72/100\n",
      "574/574 [==============================] - 0s 21us/step - loss: 0.6171 - binary_accuracy: 0.7021\n",
      "Epoch 73/100\n",
      "574/574 [==============================] - ETA: 0s - loss: 0.5800 - binary_accuracy: 0.744 - 0s 21us/step - loss: 0.6176 - binary_accuracy: 0.6864\n",
      "Epoch 74/100\n",
      "574/574 [==============================] - 0s 21us/step - loss: 0.6153 - binary_accuracy: 0.7021\n",
      "Epoch 75/100\n",
      "574/574 [==============================] - 0s 21us/step - loss: 0.6179 - binary_accuracy: 0.6864\n",
      "Epoch 76/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6156 - binary_accuracy: 0.6934\n",
      "Epoch 77/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6173 - binary_accuracy: 0.6794\n",
      "Epoch 78/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6170 - binary_accuracy: 0.6812\n",
      "Epoch 79/100\n",
      "574/574 [==============================] - 0s 21us/step - loss: 0.6159 - binary_accuracy: 0.6951\n",
      "Epoch 80/100\n",
      "574/574 [==============================] - 0s 27us/step - loss: 0.6161 - binary_accuracy: 0.6899\n",
      "Epoch 81/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6171 - binary_accuracy: 0.6882\n",
      "Epoch 82/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6179 - binary_accuracy: 0.6916\n",
      "Epoch 83/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6162 - binary_accuracy: 0.6934\n",
      "Epoch 84/100\n",
      "574/574 [==============================] - 0s 26us/step - loss: 0.6173 - binary_accuracy: 0.6916\n",
      "Epoch 85/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6159 - binary_accuracy: 0.6899\n",
      "Epoch 86/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6154 - binary_accuracy: 0.6864\n",
      "Epoch 87/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6165 - binary_accuracy: 0.6847\n",
      "Epoch 88/100\n",
      "574/574 [==============================] - 0s 25us/step - loss: 0.6163 - binary_accuracy: 0.6864\n",
      "Epoch 89/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6172 - binary_accuracy: 0.6864\n",
      "Epoch 90/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6168 - binary_accuracy: 0.6794\n",
      "Epoch 91/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6180 - binary_accuracy: 0.6882\n",
      "Epoch 92/100\n",
      "574/574 [==============================] - 0s 28us/step - loss: 0.6166 - binary_accuracy: 0.6899\n",
      "Epoch 93/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6162 - binary_accuracy: 0.6934\n",
      "Epoch 94/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6159 - binary_accuracy: 0.6934\n",
      "Epoch 95/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6168 - binary_accuracy: 0.6916\n",
      "Epoch 96/100\n",
      "574/574 [==============================] - 0s 24us/step - loss: 0.6162 - binary_accuracy: 0.6864\n",
      "Epoch 97/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6172 - binary_accuracy: 0.6864\n",
      "Epoch 98/100\n",
      "574/574 [==============================] - 0s 21us/step - loss: 0.6162 - binary_accuracy: 0.6916\n",
      "Epoch 99/100\n",
      "574/574 [==============================] - 0s 23us/step - loss: 0.6161 - binary_accuracy: 0.6916\n",
      "Epoch 100/100\n",
      "574/574 [==============================] - 0s 22us/step - loss: 0.6165 - binary_accuracy: 0.6969\n",
      "574/574 [==============================] - 0s 44us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 6)                 60        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 7         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l = len(data)\n",
    "X_train = data[:int(l*0.6)]\n",
    "X_test = data[int(l*0.6):]\n",
    "y_train = target[:int(l*0.6)]\n",
    "y_test = target[int(l*0.6):]\n",
    "model._model(X_train, y_train,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best value\n",
    "1. for number of nodes is 6.\n",
    "2. 2 layer model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
